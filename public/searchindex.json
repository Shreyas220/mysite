[{
  "section": "Blog",
  "slug": "/blog/millwheel/",
  "title": "Fault tolerance in MillWheels",
  "description": "Understanding Fault Tolerance in Streaming Systems ",
  "date": "March 10, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/millwheel1_hu_2d3d3cd09d5b4b58.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"280\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/millwheel1_hu_8137c81ff61abc3b.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/millwheel1_hu_135d23e72a30deda.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/millwheel1_hu_52afa15d13da100d.png';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Streaming Systems, Distributed Systems",
  "tags": "fault tolerance, research paper",
  "content":"Introduction Fault tolerance is super important for modern streaming systems since data is always in transit and failure can happen at any stage of the system. We can run into issues like lost messages, processing the same message twice, or keeping track of state properly. This can lead to unreliable results or even system downtime. For example, think about a system that monitors web searches to spot sudden trends. If a part of the system fails or some data gets dropped, it might miss a new trend or, even worse, trigger false alerts.\nMillWheel’s Fault Tolerance Mechanism MillWheel addressed these challenges by providing framework level fault tolerance where any node or edge in the processing graph could fail without affecting the correctness of results, it does it through a few core mechanisms:\nPersistent State Storage:\nState and metadata are continuously written to a durable store. This allows recovery to a consistent point.\nLow-Latency Recovery:\nRapid restoration of state and in-flight messages minimizes downtime. Recovery mechanisms reload state and pending messages quickly.\nIdempotent Processing:\nEach message is associated with a unique ID. Before processing, the system checks whether this event has already been handled, ensuring that duplicate processing is avoided.\nDistributed Consistency:\nMillWheel coordinates state updates across distributed nodes, ensuring that the system remains consistent even in the face of failures.\nlet\u0026rsquo;s dive a little deeper into the mechanisms\nMillWheel provides at-least-once delivery, meaning every record is retried only after it has successfully processed and acknowledged.\nThis ensures no data is lost but introduces the potential for duplicate processing, like what if a receiver might process a record, persist the new state, but then crash before sending the ACK. When the record is redelivered, the system reprocesses it, considering it a new record.\nTo prevent duplicate processing, we assign a global unique ID to each record at production time. Since keeping all processed IDs in memory would be impractical for long running systems, MillWheel employs a two-tier approach: a bloom filter for fast inmemory checking, and a persistent backing store for definitive verification\nOnce the changes are committed, the ACKs are sent to the sender so that it stops retrying.\nOnly after all processing is complete and the changes are committed does the system send acknowledgments back to senders, signaling that they can stop retrying delivery. The system also intelligently manages these record IDs, keeping them only as long as necessary. Once the system confirms that no more retries will occur (typically after internal senders have completed their retry cycles), it cleans up these duplicate detection records to conserve resources.\nNow let\u0026rsquo;s understand how this commits and recovery from the state system works\nCheckpoint commits Two Types of State for a node Hard State This is the data that’s permanently stored in a backing store (database or disk). Soft State This includes things kept in memory such as caches or temporary aggregates. MillWheel workers save their current state very often, sometimes every second or even after each record. These frequent checkpoints allow the system to recover quickly by loading the most recent saved state rather than rebuilding from scratch. Importantly, MillWheel allows workers to access their saved state without pausing all processing. This concurrent checkpointing and processing enables the system to maintain high throughput even while ensuring fault tolerance.\nTo maintain consistency across related pieces of state, MillWheel wraps state updates in atomic operations. This atomicity ensures that all parts of a key\u0026rsquo;s state are updated together, maintaining invariants and preventing partial updates. For instance, if a financial application updates both an account balance and a transaction log, these updates must succeed or fail together to prevent inconsistencies.\nDealing with “Zombie” Processes One of the most difficult challenges in distributed streaming systems involves managing \u0026ldquo;zombie\u0026rdquo; processes and preventing stale writes. When work migrates between machines, perhaps because a process failed or due to load rebalancing there\u0026rsquo;s a risk that an old or \u0026ldquo;zombie\u0026rdquo; process might later send updates that are no longer valid.\nFor example, you have a Node A working on a task, but then they get replaced by Node B because they were too slow or failed.\nMillWheel solves this through a tokenized write validation system. Each update includes a special token that verifies its validity. When a new process takes over responsibility for a key, it invalidates older tokens, ensuring that only the most current process can update that key.\nNow if Process A (the “zombie”) later tries to update the task with outdated information, the system will ignore it because Process B’s token has already taken over. The system automatically rejects these stale writes, preserving consistency.\nLets try to understand this with an example Imagine our streaming system has 4 nodes :\nSource → Ingests events Enrichment → Adds product details to each event Aggregation → Computes on events Alerting → Sends notifications for unusual patterns Now, suppose the part that crunches numbers (the Aggregation stage) suddenly crashes. Here\u0026rsquo;s what happened:\nThe server was working on a batch of 100 transactions. It finished 60 transactions before crashing. It had saved its progress (a checkpoint) after 60 transactions but did not ACK the Enrichment node for that last 10 messages . That means 10 transactions were processed after the checkpoint but not ACK to the previous node, and 40 transactions were still in the middle of being processed when the crash occurred. How the system recovers:\n1. Noticing Something’s Wrong: The system keeps an eye on all servers. When the aggregation server stops replying, the system marks it as failed and holds onto any messages that were still being processed.\n2. Restarting and Loading the Last Checkpoint: A new server instance starts up and loads the saved state from the checkpoint (which only goes up to 60 transactions).\n3. Replaying the Events: The system now replays the messages that were lost:\nThe new instance loads the state checkpoint containing data for the first 60 events It will replay from the 51th message as did not get ack after that. Which means it also replays the 10 events that were processed after the checkpoint (even though they were already processed). To avoid counting any transaction twice, every event has a unique ID. The system checks these IDs, if it sees an ID it’s already processed, it simply skips it.\nconclusion Even though the server crashed in the middle of processing, the system recovers by rolling back to the last checkpoint and reprocessing the missing events, all while ensuring that no transaction is counted twice.\n"},{
  "section": "Blog",
  "slug": "/blog/gossip/",
  "title": "Gossip Protocol",
  "description": "Understanding Gossip protocol ",
  "date": "March 10, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/image_hu_f0ce15dbb80c1249.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/image_hu_53cdd32a671cfef4.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/image_hu_a5dc81cfac81dfe.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/image_hu_2a10c2eebef76044.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Distributed Systems",
  "tags": "protocol",
  "content":"Gossip Introduction to gossip protocol\nWhy do we need gossip protocol ? Centralized Coordination Overhead\ncentralized coordinators creates bottlenecks and single points of failure. Leader failures paralyze the system until a new leader is elected Inefficient Failure Detection\nHeartbeat-based monitoring requires constant communication, consuming excessive bandwidth and overload the network but slow detection of node failures leads to stale data and service disruptions Unreliable Multicast Communication\nMulticast protocols struggle with packet loss, network splits, and scalability in large clusters. A multicast message to 1,000 nodes could overload the network and fail to reach all nodes Slow State Synchronization\nManual or slow periodic reconciliation of node states can cause delays and inconsistencies when nodes operate with outdated data, risking decision-making errors (e.g., duplicate transactions) how gossip protocol works Nodes communicate with randomly selected peers. Each node operates with limited local knowledge of the system. Communication occurs at regular intervals. The size of transmitted data is limited per gossip round. The protocol assumes network paths may fail. Interactions are infrequent to reduce overhead. Types of gossip protocol Anti Entropy Nodes periodically compare their entire dataset with other nodes to identify and rectify inconsistencies Rumor Mongering sharing only the latest updates might flood the network with frequent cycles Aggregation Gossip Protocol Dissemination Protocol Variants: Event Dissemination Protocols: Gossip periodically about events without triggering gossip directly. Background Data Dissemination Protocols: Continuously gossip about node-associated information, suitable for environments where propagation latency isn\u0026rsquo;t critical2. epidemic based protocol\nSWIM Serf (Hashicorp built on top of swim) Probabilistic Analysis of Gossip Protocols "}]
