[{
  "section": "Blog",
  "slug": "/blog/millwheel/",
  "title": "Fault tolerance in MillWheels",
  "description": "Understanding Fault Tolerance in Streaming Systems ",
  "date": "March 10, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/millwheel_hu_a40a078dbb4bcbf3.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/millwheel_hu_13ca9ce534db1e1b.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/millwheel_hu_7046309c8def065a.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/millwheel_hu_f8eeb9b235153e9a.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Streaming Systems, Distributed Systems",
  "tags": "fault tolerance, research paper",
  "content":"üöß¬†Under Construction üöß\nIntroduction Streaming systems operate under challenging conditions where data flows continuously, errors can occur at any stage, and any downtime may lead to lost or inconsistent information. This makes fault tolerance not just a feature but a necessity for ensuring that systems quickly recover from failures while maintaining state consistency and processing guarantee\nFault tolerance is a critical requirement for modern streaming systems where data flows continuously and errors can occur at any point in the processing pipeline. Traditional streaming frameworks often struggle with challenges such as message loss, duplicate processing, and state inconsistencies‚Äîproblems that can lead to unreliable results or significant downtime.\nFor example, imagine a real-time system that monitors web queries for sudden trends.\nIf a node fails or an edge in the processing graph drops data, the system might miss an emerging trend or, worse, generate false alerts.\nMillWheel‚Äôs Fault Tolerance Mechanism MillWheel addressed these challenges by providing framework-level fault tolerance where any node or edge in the processing graph could fail without affecting the correctness of results, it does it through a few core mechanisms:\nPersistent State Storage:\nState and metadata are continuously written to a durable store. This allows recovery to a consistent point.\nLow-Latency Recovery:\nRapid restoration of state and in-flight messages minimizes downtime. Recovery mechanisms reload state and pending messages quickly.\nIdempotent Processing:\nEach message is associated with a unique ID. Before processing, the system checks whether this event has already been handled, ensuring that duplicate processing is avoided.\nDistributed Consistency:\nMillWheel coordinates state updates across distributed nodes, ensuring that the system remains consistent even in the face of failures.\nlet\u0026rsquo;s dive a little deeper into the mechanisms\nMillWheel provides at-least-once delivery, meaning every record is retried only after it has successfully processed and acknowledged.\nThis ensures no data is lost but introduces the potential for duplicate processing, like what if a receiver might process a record, persist the new state, but then crash before sending the ACK. When the record is re-delivered, the system re-processes it, considering it a new record.\nTo prevent duplicate processing, we assign a global unique ID to each record at production time. Since keeping all processed IDs in memory would be impractical for long-running systems, MillWheel employs a two-tier approach: a bloom filter for fast in-memory checking, and a persistent backing store for definitive verification\nOnce the changes are committed, the ACKs are sent to the sender so that it stops retrying.\nOnly after all processing is complete and the changes are committed does the system send acknowledgments back to senders, signaling that they can stop retrying delivery. The system also intelligently manages these record IDs, keeping them only as long as necessary. Once the system confirms that no more retries will occur (typically after internal senders have completed their retry cycles), it cleans up these duplicate detection records to conserve resources.\nNow let\u0026rsquo;s understand how this commits and recovery from the state system works\nCommits Two Types of State for a node Hard State This is the data that‚Äôs permanently stored in a backing store (database or disk). Soft State This includes things kept in memory such as caches or temporary aggregates. To enable rapid recovery from failures, MillWheel workers save their current state extremely frequently‚Äîsometimes after every second or even after processing each individual record. These frequent checkpoints allow the system to recover quickly by loading the most recent saved state rather than rebuilding from scratch. Importantly, MillWheel allows workers to access their saved state without pausing all processing. This concurrent checkpointing and processing enables the system to maintain high throughput even while ensuring fault tolerance. To maintain consistency across related pieces of state, MillWheel wraps state updates in atomic operations. This atomicity ensures that all parts of a key\u0026rsquo;s state are updated together, maintaining invariants and preventing partial updates. For instance, if a financial application updates both an account balance and a transaction log, these updates must succeed or fail together to prevent inconsistencies.\nDealing with ‚ÄúZombie‚Äù Processes and Stale Writes One of the most difficult challenges in distributed streaming systems involves managing \u0026ldquo;zombie\u0026rdquo; processes and preventing stale writes. When work migrates between machines‚Äîperhaps because a process failed or due to load rebalancing‚Äîthere\u0026rsquo;s a risk that an old or \u0026ldquo;zombie\u0026rdquo; process might later send updates that are no longer valid.\nFor example, you have a Node A working on a task, but then they get replaced by Node B because they were too slow or failed.\nMillWheel solves this through a tokenized write validation system. Each update includes a special token that verifies its validity. When a new process takes over responsibility for a key, it invalidates older tokens, ensuring that only the most current process can update that key.\nNow if Process A (the ‚Äúzombie‚Äù) later tries to update the task with outdated information, the system will ignore it because Process B‚Äôs token has already taken over. The system automatically rejects these stale writes, preserving consistency.\nLets try to understand this with an example Imagine we have a simple pipeline analyzing e-commerce transactions:\nSource ‚Üí Ingests events Enrichment ‚Üí Adds product details to each event Aggregation ‚Üí Computes on events Alerting ‚Üí Sends notifications for unusual patterns Let\u0026rsquo;s say the Aggregation computation fails in the middle of processing:\nFailure Scenario A server running the Aggregation computation crashes while processing a batch of 100 events The computation had processed 60 events and updated its state accordingly 40 events were in-flight at the time of failure The last state checkpoint happened after processing 50 events Recovery Process in Detail 1. Failure Detection The MillWheel system constantly monitors the health of computation instances through periodic heartbeats. When a server fails to respond:\nThe system marks that computation instance as failed In-flight messages to the failed computation are buffered 2. State Recovery When a new instance is started:\nFor our Aggregation example:\nThe new instance loads the state checkpoint containing data for the first 50 events The state includes per-category revenue counters and any other metadata All timers (like \u0026ldquo;emit daily totals at midnight\u0026rdquo;) are restored The system identifies all messages that were sent to the failed instance but not acknowledged as processed 3. Message Replay and Deduplication This is where MillWheel\u0026rsquo;s exactly-once guarantee really shines:\nIn our example:\nThe source systems (Enrichment in this case) will replay messages that weren\u0026rsquo;t acknowledged This includes all 40 unprocessed events, but might also include some of the 10 events processed after the last checkpoint Each message in MillWheel has a unique ID The system keeps a persistent log of processed message IDs as part of its state When a message arrives, it first checks if its ID is in the \u0026ldquo;already processed\u0026rdquo; log If found, the message is a duplicate and is silently dropped If not found, the message is processed and its ID is added to the log 4. Exactly-Once Guarantee Implementation The exactly-once guarantee relies on three mechanisms working together:\nStrong Delivery: Messages are persistently stored until acknowledged Determinism: Given the same inputs and state, a computation produces the same outputs Idempotent Updates: Applying an update multiple times has the same effect as applying it once For our Aggregation computation:\n5. Maintaining Consistency During Recovery For each key/value pair in the system\u0026rsquo;s state:\nThe persistent storage maintains the latest committed value Each computation keeps track of the keys it has modified since the last checkpoint When a checkpoint occurs, only modified keys are written to persistent storage This makes checkpointing efficient while ensuring consistency For our example:\nOnly the category counters that changed during processing of those 10 events (between events 50-60) need to be recovered The system reconstructs this by replaying the events from 51-100 "},{
  "section": "Blog",
  "slug": "/blog/gossip/",
  "title": "Gossip Protocol",
  "description": "Understanding Gossip protocol ",
  "date": "March 10, 2025",
  "image": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/gossip_hu_9aef3c603f505594.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"420\"\n          height=\"315\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/gossip_hu_c402cdcc3b0322ce.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "imageSM": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n  \n  \n    \n      \n    \n\n\n    \n    \n\n\n    \n    \n      \n      \n    \n    \n    \n\n\n    \n    \n      \n      \n        \n        \n        \n        \n        \n        \n          \n          \n          \n          \n        \n\n\n        \n        \n\n\n        \u003cimg\n          \n            src=\"/images/gossip_hu_7d01022ea58e198e.webp\" loading=\"lazy\" decoding=\"async\"\n          \n\n          alt=\"\"\n          class=\"  img\"\n          width=\"100\"\n          height=\"100\"\n          onerror=\"this.onerror='null';this.src='\\/images\\/gossip_hu_5b525566159645cc.jpg';\" /\u003e\n\n        \n      \n      \n    \n    \n  \n\n\n  \n",
  "searchKeyword": "",
  "categories": "Distributed Systems",
  "tags": "protocol",
  "content":"Gossip Introduction to gossip protocol\nWhy do we need gossip protocol ? Centralized Coordination Overhead\ncentralized coordinators creates bottlenecks and single points of failure. Leader failures paralyze the system until a new leader is elected Inefficient Failure Detection\nHeartbeat-based monitoring requires constant communication, consuming excessive bandwidth and overload the network but slow detection of node failures leads to stale data and service disruptions Unreliable Multicast Communication\nMulticast protocols struggle with packet loss, network splits, and scalability in large clusters. A multicast message to 1,000 nodes could overload the network and fail to reach all nodes Slow State Synchronization\nManual or slow periodic reconciliation of node states can cause delays and inconsistencies when nodes operate with outdated data, risking decision-making errors (e.g., duplicate transactions) how gossip protocol works Nodes communicate with randomly selected peers. Each node operates with limited local knowledge of the system. Communication occurs at regular intervals. The size of transmitted data is limited per gossip round. The protocol assumes network paths may fail. Interactions are infrequent to reduce overhead. Types of gossip protocol Anti Entropy Nodes periodically compare their entire dataset with other nodes to identify and rectify inconsistencies Rumor Mongering sharing only the latest updates might flood the network with frequent cycles Aggregation Gossip Protocol Dissemination Protocol Variants: Event Dissemination Protocols: Gossip periodically about events without triggering gossip directly. Background Data Dissemination Protocols: Continuously gossip about node-associated information, suitable for environments where propagation latency isn\u0026rsquo;t critical2. epidemic based protocol\nSWIM Serf (Hashicorp built on top of swim) Probabilistic Analysis of Gossip Protocols "}]
